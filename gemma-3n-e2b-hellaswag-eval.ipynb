{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# --- Authenticate with Hugging Face ---\n# We retrieve the token stored in Kaggle Secrets.\ntry:\n    secrets = UserSecretsClient()\n    hf_token = secrets.get_secret(\"HF_TOKEN\")\n    login(token=hf_token, add_to_git_credential=False)\n    print(\"Successfully logged into Hugging Face.\")\nexcept Exception as e:\n    print(f\"Could not log in to Hugging Face. Please ensure your HF_TOKEN secret is set correctly. Error: {e}\")","metadata":{"colab_type":"code","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers.git\n!pip install --upgrade timm\n\nprint(\"Libraries installed. PLEASE RESTART YOUR RUNTIME NOW.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor, Gemma3nForConditionalGeneration\nimport transformers\nimport timm\n\n# Verify the versions to confirm the new installations are active\nprint(f\"Transformers version: {transformers.__version__}\")\nprint(f\"Timm version: {timm.__version__}\")\n\nprint(\"\\nLoading model and processor onto the first GPU (cuda:0)...\")\n\nmodel = Gemma3nForConditionalGeneration.from_pretrained(\n    \"google/gemma-3n-E2B\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda:0\" # Load to first gpu\n)\n\nprocessor = AutoProcessor.from_pretrained(\"google/gemma-3n-E2B\")\n\nprint(\"\\nModel and processor loaded successfully!\")\n# You can verify which GPU the model is on\nprint(f\"Model is on device: {model.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T13:52:40.998341Z","iopub.execute_input":"2025-07-28T13:52:40.998681Z","iopub.status.idle":"2025-07-28T13:53:31.952659Z","shell.execute_reply.started":"2025-07-28T13:52:40.998660Z","shell.execute_reply":"2025-07-28T13:53:31.951685Z"}},"outputs":[{"name":"stdout","text":"Transformers version: 4.55.0.dev0\nTimm version: 1.0.19\n\nLoading model and processor onto the first GPU (cuda:0)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18e0c1878dc40a5891492573aa99761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/196 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0927b181983f48d68cffab6da0677bf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bb91a99e6644bb3910783db2a71adfc"}},"metadata":{}},{"name":"stdout","text":"\nModel and processor loaded successfully!\nModel is on device: cuda:0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset \n# --- Load the HellaSwag benchmark dataset ---\n# We let the library automatically select the 'default' configuration for HellaSwag.\n# https://huggingface.co/datasets/Rowan/hellaswag/viewer/default/validation\ndataset = load_dataset(\"hellaswag\", split=\"validation\")\n\nprint(f\"✅ HellaSwag validation set loaded successfully. It contains {len(dataset)} examples.\")\nprint(\"\\n--- Example Entry ---\")\n# Print the first example to see the structure\nprint(f\"Context: {dataset[0]['ctx']}\")\nprint(f\"Endings: {dataset[0]['endings']}\")\nprint(f\"Correct Label: {dataset[0]['label']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T13:53:34.879904Z","iopub.execute_input":"2025-07-28T13:53:34.880461Z","iopub.status.idle":"2025-07-28T13:53:42.109149Z","shell.execute_reply.started":"2025-07-28T13:53:34.880436Z","shell.execute_reply":"2025-07-28T13:53:42.108245Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15c58fa7483345d2a06a4c633ab50b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/24.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ead21419ba446e97b39d4f31abc589"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/6.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1707498341d4f1bbe900be8a501f80f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/6.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a28714f69c41b7b55cde7881495323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc693516cd34491e98c8e95c4bc98c0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99dd1d2407f04c248639d8bfc298027b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a3dc89354724dbaa0288c6f5da9a94f"}},"metadata":{}},{"name":"stdout","text":"✅ HellaSwag validation set loaded successfully. It contains 10042 examples.\n\n--- Example Entry ---\nContext: A man is sitting on a roof. he\nEndings: ['is using wrap to wrap a pair of skis.', 'is ripping level tiles off.', \"is holding a rubik's cube.\", 'starts pulling up roofing on a roof.']\nCorrect Label: 3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- 1. Ensure the dataset sample is ready ---\nprint(f\"Full dataset has {len(dataset)} examples.\")\nsampled_dataset = dataset.shuffle(seed=42).select(range(100))\nprint(f\"Now evaluating on a random sample of {len(sampled_dataset)} examples...\")\nprint(\"-\" * 30)\n\n# --- 2. Initialize counters and start the loop ---\ncorrect_predictions = 0\ntotal_predictions = 0\n\nfrom tqdm.auto import tqdm\n\nfor example in tqdm(sampled_dataset):\n    context = example[\"ctx\"]\n    endings = example[\"endings\"]\n    correct_ending_index = int(example[\"label\"])\n\n    log_likelihoods = []\n\n    # Calculate the likelihood of each ending\n    for ending in endings:\n        input_text = context + \" \" + ending\n        \n        # --- Call the processor directly ---\n        # It returns a dictionary with 'input_ids', 'attention_mask', etc.\n        inputs = processor(text=input_text, return_tensors=\"pt\").to(model.device)\n\n        with torch.no_grad():\n            # --- Pass the entire 'inputs' dictionary to the model ---\n            # The **inputs syntax unpacks the dictionary into arguments (input_ids=..., attention_mask=...)\n            # We also provide the labels for loss calculation.\n            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n            \n            log_likelihood = -outputs.loss.item()\n            log_likelihoods.append(log_likelihood)\n\n    # The model's prediction is the ending with the highest log likelihood\n    predicted_index = log_likelihoods.index(max(log_likelihoods))\n\n    # Check if the prediction was correct\n    if predicted_index == correct_ending_index:\n        correct_predictions += 1\n    \n    total_predictions += 1\n \nprint(\"Evaluation loop finished.\")\n\n# --- 3. Calculate and print the final accuracy ---\naccuracy = (correct_predictions / total_predictions) * 100\nprint(\"-\" * 30)\nprint(f\"Evaluation finished on {total_predictions} examples.\")\nprint(f\"Correct predictions: {correct_predictions}\")\nprint(f\"Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T13:57:35.723028Z","iopub.execute_input":"2025-07-28T13:57:35.723305Z","iopub.status.idle":"2025-07-28T13:59:37.723206Z","shell.execute_reply.started":"2025-07-28T13:57:35.723287Z","shell.execute_reply":"2025-07-28T13:59:37.722256Z"}},"outputs":[{"name":"stdout","text":"Full dataset has 10042 examples.\nNow evaluating on a random sample of 100 examples...\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582bb11d22fc43cc85249a7f5ca788b6"}},"metadata":{}},{"name":"stdout","text":"Evaluation loop finished.\n------------------------------\nEvaluation finished on 100 examples.\nCorrect predictions: 64\nAccuracy: 64.00%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- 1. Define the number of samples for evaluation ---\nnum_eval_samples = 1000\n\nprint(f\"Full dataset has {len(dataset)} examples.\")\n\n# Shuffle the dataset and select a new, larger random sample.\nsampled_dataset_1000 = dataset.shuffle(seed=42).select(range(num_eval_samples))\n\nprint(f\"Creating a new evaluation run on {len(sampled_dataset_1000)} random examples...\")\nprint(\"-\" * 30)\n\n\n# --- 2. Initialize counters and start the loop ---\ncorrect_predictions = 0\ntotal_predictions = 0\n\nfrom tqdm.auto import tqdm\n\n# Loop over the new 1000-example sub-dataset\nfor example in tqdm(sampled_dataset_1000):\n    context = example[\"ctx\"]\n    endings = example[\"endings\"]\n    correct_ending_index = int(example[\"label\"])\n\n    log_likelihoods = []\n\n    # Calculate the likelihood of each ending\n    for ending in endings:\n        input_text = context + \" \" + ending\n        \n        # Call the processor to tokenize the text and move to the model's device\n        inputs = processor(text=input_text, return_tensors=\"pt\").to(model.device)\n\n        with torch.no_grad():\n            # Pass the processor's output to the model to get the loss\n            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n            log_likelihood = -outputs.loss.item()\n            log_likelihoods.append(log_likelihood)\n\n    # The model's prediction is the ending with the highest log likelihood\n    predicted_index = log_likelihoods.index(max(log_likelihoods))\n\n    # Check if the prediction was correct\n    if predicted_index == correct_ending_index:\n        correct_predictions += 1\n    \n    total_predictions += 1\n \nprint(\"Evaluation loop finished.\")\n\n# --- 3. Calculate and print the final accuracy for this run ---\naccuracy = (correct_predictions / total_predictions) * 100\nprint(\"-\" * 30)\nprint(f\"Evaluation finished on {total_predictions} examples.\")\nprint(f\"Correct predictions: {correct_predictions}\")\nprint(f\"Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T13:59:56.571037Z","iopub.execute_input":"2025-07-28T13:59:56.571657Z","iopub.status.idle":"2025-07-28T14:20:22.231597Z","shell.execute_reply.started":"2025-07-28T13:59:56.571630Z","shell.execute_reply":"2025-07-28T14:20:22.230750Z"}},"outputs":[{"name":"stdout","text":"Full dataset has 10042 examples.\nCreating a new evaluation run on 1000 random examples...\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b019d3a9414ba2828138113866ca2d"}},"metadata":{}},{"name":"stdout","text":"Evaluation loop finished.\n------------------------------\nEvaluation finished on 1000 examples.\nCorrect predictions: 668\nAccuracy: 66.80%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- 1. Prepare for full dataset evaluation ---\nprint(f\"Starting evaluation on the full dataset of {len(dataset)} examples.\")\nprint(\"-\" * 30)\n\n\n# --- 2. Initialize counters and start the loop ---\ncorrect_predictions = 0\ntotal_predictions = 0\n\nfrom tqdm.auto import tqdm\n\n# Loop over the ENTIRE dataset. No sampling is needed.\nfor example in tqdm(dataset):\n    context = example[\"ctx\"]\n    endings = example[\"endings\"]\n    correct_ending_index = int(example[\"label\"])\n\n    log_likelihoods = []\n\n    # Calculate the likelihood of each ending\n    for ending in endings:\n        input_text = context + \" \" + ending\n        \n        # Call the processor to tokenize the text and move to the model's device\n        inputs = processor(text=input_text, return_tensors=\"pt\").to(model.device)\n\n        with torch.no_grad():\n            # Pass the processor's output to the model to get the loss\n            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n            log_likelihood = -outputs.loss.item()\n            log_likelihoods.append(log_likelihood)\n\n    # The model's prediction is the ending with the highest log likelihood\n    predicted_index = log_likelihoods.index(max(log_likelihoods))\n\n    # Check if the prediction was correct\n    if predicted_index == correct_ending_index:\n        correct_predictions += 1\n    \n    total_predictions += 1\n \nprint(\"Full evaluation loop finished.\")\n\n# --- 3. Calculate and print the final accuracy ---\naccuracy = (correct_predictions / total_predictions) * 100\nprint(\"-\" * 30)\nprint(f\"Evaluation finished on {total_predictions} examples.\")\nprint(f\"Correct predictions: {correct_predictions}\")\nprint(f\"Final Model Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:20:55.594765Z","iopub.execute_input":"2025-07-28T14:20:55.595088Z","iopub.status.idle":"2025-07-28T17:45:59.503322Z","shell.execute_reply.started":"2025-07-28T14:20:55.595067Z","shell.execute_reply":"2025-07-28T17:45:59.502426Z"}},"outputs":[{"name":"stdout","text":"Starting evaluation on the full dataset of 10042 examples.\n------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10042 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076dfe8721af4930897739cd10475d7e"}},"metadata":{}},{"name":"stdout","text":"Full evaluation loop finished.\n------------------------------\nEvaluation finished on 10042 examples.\nCorrect predictions: 6902\nFinal Model Accuracy: 68.73%\n","output_type":"stream"}],"execution_count":9}]}